{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brokoli bagus untuk dimakan. Adikku suka makan brokoli, tetapi ibuku tidak.\"\n",
    "doc_b = \"Ibuku menghabiskan banyak waktu berkeliling melihat latihan bisbol adikku.\"\n",
    "doc_c = \"Beberapa ahli kesehatan menyarankan bahwa mengemudi dapat menyebabkan ketegangan dan tekanan darah meningkat.\"\n",
    "doc_d = \"Saya sering merasakan tekanan untuk tampil baik saat presentasi di sekolah.\"\n",
    "doc_e = \"Profesional kesehatan mengatakan bahwa brokoli itu baik untuk kesehatan Anda.\"\n",
    "doc_f = \"Teman saya seorang pemain bisbol yang pernah mendapatkan juara.\"\n",
    "doc_g = \"Pemain bisbol yang bernama Flash itu sangat suka memakan brokoli.\"\n",
    "doc_h = \"Sopir yang mengemudi taksi itu mendapatkan tekanan dari penumpangnya.\"\n",
    "doc_i = \"Saat tanding, olahraga bisbol memberikan ketegangan dan meningkatkan tekanan darah para penonton.\"\n",
    "doc_j = \"Ibuku menyarankan saya untuk memakan brokoli agar tekanan darah terkontrol.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hmmtagger.tagger import MainTagger\n",
    "from tokenization import *\n",
    "\n",
    "mt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daftar Tagging\n",
    "Berikut adalah daftar tagging berdasarkan jurnal **HMM Based Part-of-Speech Tagger for Bahasa Indonesia.** (Alfan Farizki & Ayu Purwarianti, 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>POS Name</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OP</td>\n",
       "      <td>Open Parenthesis</td>\n",
       "      <td>({[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CP</td>\n",
       "      <td>Close Parenthesis</td>\n",
       "      <td>)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GM</td>\n",
       "      <td>Slash</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>;</td>\n",
       "      <td>Semicolon</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:</td>\n",
       "      <td>Colon</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"</td>\n",
       "      <td>Quotation</td>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>Sentence Terminator</td>\n",
       "      <td>.!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>Comma</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-</td>\n",
       "      <td>Dash</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>...</td>\n",
       "      <td>Ellipsis</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>Kaya, Manis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RB</td>\n",
       "      <td>Adverb</td>\n",
       "      <td>Sementara, Nanti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN</td>\n",
       "      <td>Common Noun</td>\n",
       "      <td>Mobil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NNP</td>\n",
       "      <td>Proper Noun</td>\n",
       "      <td>Bekasi, Indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NNG</td>\n",
       "      <td>Genitive Noun</td>\n",
       "      <td>Bukunya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VBI</td>\n",
       "      <td>Intransitive Verb</td>\n",
       "      <td>Pergi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VBT</td>\n",
       "      <td>Transitive Verb</td>\n",
       "      <td>Membeli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>IN</td>\n",
       "      <td>Preposition</td>\n",
       "      <td>Di, Ke, Dari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MD</td>\n",
       "      <td>Modal</td>\n",
       "      <td>Bisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CC</td>\n",
       "      <td>Coor-Conjuction</td>\n",
       "      <td>Dan, Atau, Tetapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SC</td>\n",
       "      <td>Subor-Conjunction</td>\n",
       "      <td>Jika, Ketika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "      <td>Para, Ini, Itu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>UH</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>Wah, Aduh, Oi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CDO</td>\n",
       "      <td>Ordinal Numerals</td>\n",
       "      <td>Pertama, Kedua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CDC</td>\n",
       "      <td>Collective Numerals</td>\n",
       "      <td>Bertiga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CDP</td>\n",
       "      <td>Primary Numerals</td>\n",
       "      <td>Satu, Dua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CDI</td>\n",
       "      <td>Irregular Numerals</td>\n",
       "      <td>Beberapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal Pronouns</td>\n",
       "      <td>Saya, Kamu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WP</td>\n",
       "      <td>WH-Pronouns</td>\n",
       "      <td>Apa, Siapa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PRN</td>\n",
       "      <td>Number Pronouns</td>\n",
       "      <td>Kedua-duanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PRL</td>\n",
       "      <td>Locative Pronouns</td>\n",
       "      <td>Sini, Situ, Sana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NEG</td>\n",
       "      <td>Negation</td>\n",
       "      <td>Bukan, Tidak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SYM</td>\n",
       "      <td>Symbol</td>\n",
       "      <td>@#$%^&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RP</td>\n",
       "      <td>Particles</td>\n",
       "      <td>Pun, Kah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>FW</td>\n",
       "      <td>Foreign Word</td>\n",
       "      <td>Foreign, Word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    POS             POS Name            Example\n",
       "1    OP     Open Parenthesis                ({[\n",
       "2    CP    Close Parenthesis                )}]\n",
       "3    GM                Slash                  /\n",
       "4     ;            Semicolon                  ;\n",
       "5     :                Colon                  :\n",
       "6     \"            Quotation                  \"\n",
       "7     .  Sentence Terminator                .!?\n",
       "8     ,                Comma                  ,\n",
       "9     -                 Dash                  -\n",
       "10  ...             Ellipsis                ...\n",
       "11   JJ            Adjective        Kaya, Manis\n",
       "12   RB               Adverb   Sementara, Nanti\n",
       "13   NN          Common Noun              Mobil\n",
       "14  NNP          Proper Noun  Bekasi, Indonesia\n",
       "15  NNG        Genitive Noun            Bukunya\n",
       "16  VBI    Intransitive Verb              Pergi\n",
       "17  VBT      Transitive Verb            Membeli\n",
       "18   IN          Preposition       Di, Ke, Dari\n",
       "19   MD                Modal               Bisa\n",
       "20   CC      Coor-Conjuction  Dan, Atau, Tetapi\n",
       "21   SC    Subor-Conjunction       Jika, Ketika\n",
       "22   DT           Determiner     Para, Ini, Itu\n",
       "23   UH         Interjection      Wah, Aduh, Oi\n",
       "24  CDO     Ordinal Numerals     Pertama, Kedua\n",
       "25  CDC  Collective Numerals            Bertiga\n",
       "26  CDP     Primary Numerals          Satu, Dua\n",
       "27  CDI   Irregular Numerals           Beberapa\n",
       "28  PRP    Personal Pronouns         Saya, Kamu\n",
       "29   WP          WH-Pronouns         Apa, Siapa\n",
       "30  PRN      Number Pronouns       Kedua-duanya\n",
       "31  PRL    Locative Pronouns   Sini, Situ, Sana\n",
       "32  NEG             Negation       Bukan, Tidak\n",
       "33  SYM               Symbol             @#$%^&\n",
       "34   RP            Particles           Pun, Kah\n",
       "35   FW         Foreign Word      Foreign, Word"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# daftar tagging\n",
    "df = pd.DataFrame({'POS' : ['OP', 'CP', 'GM', ';', ':', \n",
    "                            '\"', '.', ',', '-', '...', \n",
    "                            'JJ', 'RB', 'NN', 'NNP','NNG', \n",
    "                            'VBI', 'VBT', 'IN', 'MD', 'CC', \n",
    "                            'SC', 'DT', 'UH', 'CDO', 'CDC', \n",
    "                            'CDP', 'CDI', 'PRP', 'WP', 'PRN', \n",
    "                            'PRL', 'NEG', 'SYM', 'RP', 'FW'],\n",
    "                   'POS Name' : ['Open Parenthesis', 'Close Parenthesis', 'Slash', 'Semicolon', 'Colon', \n",
    "                                 'Quotation', 'Sentence Terminator', 'Comma', 'Dash', 'Ellipsis', \n",
    "                                 'Adjective', 'Adverb', 'Common Noun', 'Proper Noun', 'Genitive Noun', \n",
    "                                 'Intransitive Verb', 'Transitive Verb', 'Preposition', 'Modal', 'Coor-Conjuction', \n",
    "                                 'Subor-Conjunction', 'Determiner', 'Interjection', 'Ordinal Numerals', 'Collective Numerals', \n",
    "                                 'Primary Numerals', 'Irregular Numerals', 'Personal Pronouns', 'WH-Pronouns', 'Number Pronouns',\n",
    "                                 'Locative Pronouns', 'Negation', 'Symbol', 'Particles', 'Foreign Word'],\n",
    "                   'Example' : ['({[', ')}]', '/', ';', ':',\n",
    "                                '\"', '.!?', ',', '-', '...',\n",
    "                                'Kaya, Manis', 'Sementara, Nanti', 'Mobil', 'Bekasi, Indonesia', 'Bukunya',\n",
    "                                'Pergi', 'Membeli', 'Di, Ke, Dari', 'Bisa', 'Dan, Atau, Tetapi',\n",
    "                                'Jika, Ketika', 'Para, Ini, Itu', 'Wah, Aduh, Oi', 'Pertama, Kedua', 'Bertiga',\n",
    "                                'Satu, Dua', 'Beberapa', 'Saya, Kamu', 'Apa, Siapa', 'Kedua-duanya',\n",
    "                                'Sini, Situ, Sana', 'Bukan, Tidak', '@#$%^&', 'Pun, Kah', 'Foreign, Word']})\n",
    "\n",
    "df = df[['POS', 'POS Name', 'Example']]\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembagian Tagging\n",
    "Dibagi menjadi 2 yaitu **Class Content** dan **Class Function**.\n",
    "\n",
    "**Class Content:**\n",
    "1. JJ = Adjective\n",
    "2. NN = Common Noun\n",
    "3. NNP = Proper Noun\n",
    "4. NNG = Genitive Noun\n",
    "5. VBI = Intransitive Verb\n",
    "6. VBT = Transitive Verb\n",
    "7. FW = Foreign Word\n",
    "\n",
    "\n",
    "**Class Function:**\n",
    "1. OP = Open Parenthesis\n",
    "2. CP = Close Parenthesis\n",
    "3. GM = Slash\n",
    "4. ; = Semicolon\n",
    "5. : = Colon\n",
    "6. \" = Quotation\n",
    "7. . = Sentence Terminator\n",
    "8. , = Comma\n",
    "9. '-' = Dash\n",
    "10. ... = Ellipsis\n",
    "11. RB = Adverb\n",
    "12. IN = Preposition\n",
    "13. MD = Modal\n",
    "14. CC = Coor-Conjunction\n",
    "15. SC = Subor-Conjunction\n",
    "16. DT = Determiner\n",
    "17. UH = Interjection\n",
    "18. CDO = Ordinal Numerals\n",
    "19. CDC = Collective Numerals\n",
    "20. CDP = Primary Numerals\n",
    "21. CDI = Irregular Numerals\n",
    "22. PRP = Personal Pronouns\n",
    "23. WP = WH-Pronouns\n",
    "24. PRN = Number Pronouns\n",
    "25. PRL = Locative Pronouns\n",
    "26. NEG = Negation\n",
    "27. SYM = Symbol\n",
    "28. RP = Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ccon = ['JJ', 'NN','NNP', 'NNG', 'VBI', 'VBT', 'FW']\n",
    "Cfunc = ['OP', 'CP', 'GM', ';', ':', '\"', '.', \n",
    "         ',', '-', '...', 'RB', 'IN', 'MD', 'CC',\n",
    "         'SC', 'DT', 'UH', 'CDO', 'CDC', 'CDP', 'CDI',\n",
    "         'PRP', 'WP', 'PRN', 'PRL', 'NEG', 'SYM', 'RP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    String fileLexicon\n",
    "    String fileNGram\n",
    "    int NGramType = 0\n",
    "    int maxAffixLength = 3\n",
    "    int Treshold = 3\n",
    "    int minWordFreq = 0\n",
    "    int modeAffixTree = 0\n",
    "    boolean debug = False\n",
    "    double LambdaBigram = 0.2\n",
    "    int TwoPhaseType = 0\n",
    "    double beamFactor = 500.0\n",
    "    int useLexicon = 0\n",
    "\"\"\"\n",
    "\n",
    "def init_tag():\n",
    "    global mt\n",
    "    try:\n",
    "        if mt is None:\n",
    "            mt = MainTagger(\"resource/Lexicon.trn\", \"resource/Ngram.trn\", 0, 3, 3, 0, 0, False, 0.2, 0, 500.0, 1)\n",
    "    except:\n",
    "        print(\"Error Exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Saya suka makan bakso H. Hadi. Baksonya sangat enak dan mantap. Aku kepedesan hehe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saya suka makan bakso H. Hadi.',\n",
       " 'Baksonya sangat enak dan mantap.',\n",
       " 'Aku kepedesan hehe.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sentence_extraction(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aku kepedesan hehe .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in a:\n",
    "    hasil = \" \".join(tokenisasi_kalimat(_)).strip()\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brokoli/NN', 'bagus/NN', 'untuk/IN', 'dimakan/VBT', './.']\n",
      "['Adikku/NN', 'suka/VBI', 'makan/VBT', 'brokoli/NN', ',/,', 'tetapi/CC', 'ibuku/NN', 'tidak/NEG', './.']\n",
      "['Ibuku/NN', 'menghabiskan/VBT', 'banyak/JJ', 'waktu/NN', 'berkeliling/NN', 'melihat/VBT', 'latihan/NN', 'bisbol/NN', 'adikku/NNP', './.']\n",
      "['Beberapa/CDI', 'ahli/NN', 'kesehatan/NN', 'menyarankan/VBT', 'bahwa/SC', 'mengemudi/VBT', 'dapat/MD', 'menyebabkan/VBT', 'ketegangan/NN', 'dan/CC', 'tekanan/NN', 'darah/NN', 'meningkat/VBI', './.']\n",
      "['Saya/PRP', 'sering/JJ', 'merasakan/VBT', 'tekanan/NN', 'untuk/IN', 'tampil/NN', 'baik/JJ', 'saat/SC', 'presentasi/NN', 'di/IN', 'sekolah/NN', './.']\n",
      "['Profesional/NNP', 'kesehatan/NN', 'mengatakan/VBI', 'bahwa/SC', 'brokoli/NN', 'itu/DT', 'baik/JJ', 'untuk/IN', 'kesehatan/NN', 'Anda/PRP', './.']\n",
      "['Teman/NN', 'saya/PRP', 'seorang/NN', 'pemain/NN', 'bisbol/NN', 'yang/SC', 'pernah/NN', 'mendapatkan/VBT', 'juara/NN', './.']\n",
      "['Pemain/NN', 'bisbol/NN', 'yang/SC', 'bernama/VBT', 'Flash/NN', 'itu/DT', 'sangat/RB', 'suka/VBI', 'memakan/VBT', 'brokoli/NN', './.']\n",
      "['Sopir/NNP', 'yang/SC', 'mengemudi/VBT', 'taksi/NEG', 'itu/DT', 'mendapatkan/VBT', 'tekanan/NN', 'dari/IN', 'penumpangnya/NN', './.']\n",
      "['Saat/NN', 'tanding/NN', ',/,', 'olahraga/NN', 'bisbol/MD', 'memberikan/VBT', 'ketegangan/NN', 'dan/CC', 'meningkatkan/VBT', 'tekanan/NN', 'darah/NN', 'para/DT', 'penonton/NN', './.']\n",
      "['Ibuku/NN', 'menyarankan/VBT', 'saya/PRP', 'untuk/IN', 'memakan/VBT', 'brokoli/NN', 'agar/IN', 'tekanan/NN', 'darah/NN', 'terkontrol/JJ', './.']\n"
     ]
    }
   ],
   "source": [
    "# tokenisasi dan tagging\n",
    "tagged_doc = []\n",
    "for doc in documents:\n",
    "    lines = doc.strip().split(\"\\n\")\n",
    "    try:\n",
    "        init_tag()\n",
    "        for l in lines:\n",
    "            if len(l) == 0: continue\n",
    "            out = sentence_extraction(cleaning(l))\n",
    "            for o in out:\n",
    "                strtag = \" \".join(tokenisasi_kalimat(o)).strip()\n",
    "#                 result += [\" \".join(mt.taggingStr(strtag))]\n",
    "                tagged_doc += [mt.taggingStr(strtag)]\n",
    "    except:\n",
    "        print (\"Error Exception\")\n",
    "\n",
    "for _ in tagged_doc:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tagging buat cari topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brokoli/NN', 'bagus/NN', 'dimakan/VBT']\n",
      "['Adikku/NN', 'suka/VBI', 'makan/VBT', 'brokoli/NN', 'ibuku/NN']\n",
      "['Ibuku/NN', 'menghabiskan/VBT', 'banyak/JJ', 'waktu/NN', 'berkeliling/NN', 'melihat/VBT', 'latihan/NN', 'bisbol/NN', 'adikku/NNP']\n",
      "['ahli/NN', 'kesehatan/NN', 'menyarankan/VBT', 'mengemudi/VBT', 'menyebabkan/VBT', 'ketegangan/NN', 'tekanan/NN', 'darah/NN', 'meningkat/VBI']\n",
      "['sering/JJ', 'merasakan/VBT', 'tekanan/NN', 'tampil/NN', 'baik/JJ', 'presentasi/NN', 'sekolah/NN']\n",
      "['Profesional/NNP', 'kesehatan/NN', 'mengatakan/VBI', 'brokoli/NN', 'baik/JJ', 'kesehatan/NN']\n",
      "['Teman/NN', 'seorang/NN', 'pemain/NN', 'bisbol/NN', 'pernah/NN', 'mendapatkan/VBT', 'juara/NN']\n",
      "['Pemain/NN', 'bisbol/NN', 'bernama/VBT', 'Flash/NN', 'suka/VBI', 'memakan/VBT', 'brokoli/NN']\n",
      "['Sopir/NNP', 'mengemudi/VBT', 'mendapatkan/VBT', 'tekanan/NN', 'penumpangnya/NN']\n",
      "['Saat/NN', 'tanding/NN', 'olahraga/NN', 'memberikan/VBT', 'ketegangan/NN', 'meningkatkan/VBT', 'tekanan/NN', 'darah/NN', 'penonton/NN']\n",
      "['Ibuku/NN', 'menyarankan/VBT', 'memakan/VBT', 'brokoli/NN', 'tekanan/NN', 'darah/NN', 'terkontrol/JJ']\n"
     ]
    }
   ],
   "source": [
    "doc_con = []\n",
    "for tagged in tagged_doc:\n",
    "    con = []\n",
    "    for _ in tagged:\n",
    "        if _.split(\"/\", 1)[1] in Ccon:\n",
    "            con += [\"\".join(_)]\n",
    "    doc_con += [con]\n",
    "\n",
    "for _ in doc_con:\n",
    "    print (_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brokoli', 'bagus', 'dimakan']\n",
      "['Adikku', 'suka', 'makan', 'brokoli', 'ibuku']\n",
      "['Ibuku', 'menghabiskan', 'banyak', 'waktu', 'berkeliling', 'melihat', 'latihan', 'bisbol', 'adikku']\n",
      "['ahli', 'kesehatan', 'menyarankan', 'mengemudi', 'menyebabkan', 'ketegangan', 'tekanan', 'darah', 'meningkat']\n",
      "['sering', 'merasakan', 'tekanan', 'tampil', 'baik', 'presentasi', 'sekolah']\n",
      "['Profesional', 'kesehatan', 'mengatakan', 'brokoli', 'baik', 'kesehatan']\n",
      "['Teman', 'seorang', 'pemain', 'bisbol', 'pernah', 'mendapatkan', 'juara']\n",
      "['Pemain', 'bisbol', 'bernama', 'Flash', 'suka', 'memakan', 'brokoli']\n",
      "['Sopir', 'mengemudi', 'mendapatkan', 'tekanan', 'penumpangnya']\n",
      "['Saat', 'tanding', 'olahraga', 'memberikan', 'ketegangan', 'meningkatkan', 'tekanan', 'darah', 'penonton']\n",
      "['Ibuku', 'menyarankan', 'memakan', 'brokoli', 'tekanan', 'darah', 'terkontrol']\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for con in doc_con:\n",
    "    co = []\n",
    "    for c in con:\n",
    "        result = c.split('/', 1)[0]\n",
    "        co.append(result)\n",
    "    documents += [co]\n",
    "    \n",
    "for _ in documents:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n"
     ]
    }
   ],
   "source": [
    "text = 'name/NN'\n",
    "sep = '/'\n",
    "rest = text.split(sep, 1)[0]\n",
    "print (rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBI\n"
     ]
    }
   ],
   "source": [
    "my_string=\"mengatakan/VBI\"\n",
    "print (my_string.split(\"/\",1)[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()     # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                      # return the smallest i such that\n",
    "        if rnd <= 0: return i         # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each documents\n",
    "document_lengths = [len(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of distinct words\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of documents\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "    \n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fration of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "    \n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "    \n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic, in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            \n",
    "            # remove this word / topic from counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tekanan 4\n",
      "0 darah 3\n",
      "0 ketegangan 2\n",
      "0 menyarankan 2\n",
      "0 Saat 1\n",
      "0 tanding 1\n",
      "0 meningkatkan 1\n",
      "0 olahraga 1\n",
      "0 memberikan 1\n",
      "0 penonton 1\n",
      "0 penumpangnya 1\n",
      "0 meningkat 1\n",
      "0 ahli 1\n",
      "1 Ibuku 2\n",
      "1 waktu 1\n",
      "1 bisbol 1\n",
      "1 banyak 1\n",
      "1 Brokoli 1\n",
      "1 dimakan 1\n",
      "1 terkontrol 1\n",
      "1 menghabiskan 1\n",
      "1 latihan 1\n",
      "1 adikku 1\n",
      "1 presentasi 1\n",
      "2 brokoli 4\n",
      "2 bisbol 2\n",
      "2 memakan 2\n",
      "2 suka 2\n",
      "2 Adikku 1\n",
      "2 Flash 1\n",
      "2 Profesional 1\n",
      "2 bagus 1\n",
      "2 menyebabkan 1\n",
      "2 Pemain 1\n",
      "2 bernama 1\n",
      "3 kesehatan 3\n",
      "3 baik 2\n",
      "3 sering 1\n",
      "3 mengatakan 1\n",
      "3 merasakan 1\n",
      "3 mengemudi 1\n",
      "3 sekolah 1\n",
      "3 tampil 1\n",
      "3 melihat 1\n",
      "3 berkeliling 1\n",
      "4 mendapatkan 2\n",
      "4 mengemudi 1\n",
      "4 seorang 1\n",
      "4 juara 1\n",
      "4 tekanan 1\n",
      "4 makan 1\n",
      "4 ibuku 1\n",
      "4 Sopir 1\n",
      "4 pernah 1\n",
      "4 Teman 1\n",
      "4 pemain 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print (k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = [\"Topik 1: \",\n",
    "               \"Topik 2: \",\n",
    "               \"Topik 3: \",\n",
    "               \"Topik 4: \",\n",
    "               \"Topik 5: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brokoli', 'bagus', 'dimakan']\n",
      "Topik 2:  2\n",
      "Topik 3:  1\n",
      "['Adikku', 'suka', 'makan', 'brokoli', 'ibuku']\n",
      "Topik 3:  3\n",
      "Topik 5:  2\n",
      "['Ibuku', 'menghabiskan', 'banyak', 'waktu', 'berkeliling', 'melihat', 'latihan', 'bisbol', 'adikku']\n",
      "Topik 2:  7\n",
      "Topik 4:  2\n",
      "['ahli', 'kesehatan', 'menyarankan', 'mengemudi', 'menyebabkan', 'ketegangan', 'tekanan', 'darah', 'meningkat']\n",
      "Topik 1:  6\n",
      "Topik 4:  2\n",
      "Topik 3:  1\n",
      "['sering', 'merasakan', 'tekanan', 'tampil', 'baik', 'presentasi', 'sekolah']\n",
      "Topik 4:  5\n",
      "Topik 1:  1\n",
      "Topik 2:  1\n",
      "['Profesional', 'kesehatan', 'mengatakan', 'brokoli', 'baik', 'kesehatan']\n",
      "Topik 4:  4\n",
      "Topik 3:  2\n",
      "['Teman', 'seorang', 'pemain', 'bisbol', 'pernah', 'mendapatkan', 'juara']\n",
      "Topik 5:  6\n",
      "Topik 3:  1\n",
      "['Pemain', 'bisbol', 'bernama', 'Flash', 'suka', 'memakan', 'brokoli']\n",
      "Topik 3:  7\n",
      "['Sopir', 'mengemudi', 'mendapatkan', 'tekanan', 'penumpangnya']\n",
      "Topik 5:  4\n",
      "Topik 1:  1\n",
      "['Saat', 'tanding', 'olahraga', 'memberikan', 'ketegangan', 'meningkatkan', 'tekanan', 'darah', 'penonton']\n",
      "Topik 1:  9\n",
      "['Ibuku', 'menyarankan', 'memakan', 'brokoli', 'tekanan', 'darah', 'terkontrol']\n",
      "Topik 1:  3\n",
      "Topik 3:  2\n",
      "Topik 2:  2\n"
     ]
    }
   ],
   "source": [
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count, in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "topics = {'type': 'corpus',\n",
    "          'name': 'corpus1',\n",
    "          'docs': [{'name':'doc1',\n",
    "                   'topics':[{'name': 'docker' ,\n",
    "                              'words':['docker', 'containers', 'virtualidanzation']},\n",
    "                             {'name': 'windows' ,\n",
    "                              'words':['windows', 'machine', 'virtualization']},\n",
    "                             {'name': 'linux' ,\n",
    "                              'words':['linux', 'machine', 'virtualization']}\n",
    "                            ]\n",
    "                  },\n",
    "                  {'name':'doc2',\n",
    "                   'topics':[{'name': 'arraylist' ,\n",
    "                              'words':['arraylist', 'object', 'size']},\n",
    "                             {'name': 'java' ,\n",
    "                              'words':['java', 'arraylist']}\n",
    "                            ]\n",
    "                  }]\n",
    "          }\n",
    "          \n",
    "dicti = {'doc1': {'docker' : ['docker', 'containers', 'virtualization'],\n",
    "                    'windows' : ['windows', 'machine', 'virtualization'],\n",
    "                    'linux' : ['linux', 'machine', 'virtualization']\n",
    "                    },\n",
    "          'doc2': {'arraylist' : ['arraylist', 'object', 'size'],\n",
    "                    'java' : ['java', 'arraylist']\n",
    "                    }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-07d953c401eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object() takes no parameters"
     ]
    }
   ],
   "source": [
    "lda = LdaModel(documents, 5, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
