{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brokoli bagus untuk dimakan. Adikku suka makan brokoli, tetapi ibuku tidak.\"\n",
    "doc_b = \"Ibuku menghabiskan banyak waktu berkeliling melihat latihan bisbol adikku.\"\n",
    "doc_c = \"Beberapa ahli kesehatan menyarankan bahwa mengemudi dapat menyebabkan ketegangan dan tekanan darah meningkat.\"\n",
    "doc_d = \"Saya sering merasakan tekanan untuk tampil baik saat presentasi di sekolah.\"\n",
    "doc_e = \"Profesional kesehatan mengatakan bahwa brokoli itu baik untuk kesehatan Anda.\"\n",
    "doc_f = \"Teman saya seorang pemain bisbol yang pernah mendapatkan juara.\"\n",
    "doc_g = \"Pemain bisbol yang bernama Flash itu sangat suka memakan brokoli.\"\n",
    "doc_h = \"Sopir yang mengemudi taksi itu mendapatkan tekanan dari penumpangnya.\"\n",
    "doc_i = \"Saat tanding, olahraga bisbol memberikan ketegangan dan meningkatkan tekanan darah para penonton.\"\n",
    "doc_j = \"Ibuku menyarankan saya untuk memakan brokoli agar tekanan darah terkontrol.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hmmtagger.tagger import MainTagger\n",
    "from tokenization import *\n",
    "\n",
    "mt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daftar Tagging\n",
    "Berikut adalah daftar tagging berdasarkan jurnal **HMM Based Part-of-Speech Tagger for Bahasa Indonesia.** (Alfan Farizki & Ayu Purwarianti, 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daftar tagging\n",
    "df = pd.DataFrame({'POS' : ['OP', 'CP', 'GM', ';', ':', \n",
    "                            '\"', '.', ',', '-', '...', \n",
    "                            'JJ', 'RB', 'NN', 'NNP','NNG', \n",
    "                            'VBI', 'VBT', 'IN', 'MD', 'CC', \n",
    "                            'SC', 'DT', 'UH', 'CDO', 'CDC', \n",
    "                            'CDP', 'CDI', 'PRP', 'WP', 'PRN', \n",
    "                            'PRL', 'NEG', 'SYM', 'RP', 'FW'],\n",
    "                   'POS Name' : ['Open Parenthesis', 'Close Parenthesis', 'Slash', 'Semicolon', 'Colon', \n",
    "                                 'Quotation', 'Sentence Terminator', 'Comma', 'Dash', 'Ellipsis', \n",
    "                                 'Adjective', 'Adverb', 'Common Noun', 'Proper Noun', 'Genitive Noun', \n",
    "                                 'Intransitive Verb', 'Transitive Verb', 'Preposition', 'Modal', 'Coor-Conjuction', \n",
    "                                 'Subor-Conjunction', 'Determiner', 'Interjection', 'Ordinal Numerals', 'Collective Numerals', \n",
    "                                 'Primary Numerals', 'Irregular Numerals', 'Personal Pronouns', 'WH-Pronouns', 'Number Pronouns',\n",
    "                                 'Locative Pronouns', 'Negation', 'Symbol', 'Particles', 'Foreign Word'],\n",
    "                   'Example' : ['({[', ')}]', '/', ';', ':',\n",
    "                                '\"', '.!?', ',', '-', '...',\n",
    "                                'Kaya, Manis', 'Sementara, Nanti', 'Mobil', 'Bekasi, Indonesia', 'Bukunya',\n",
    "                                'Pergi', 'Membeli', 'Di, Ke, Dari', 'Bisa', 'Dan, Atau, Tetapi',\n",
    "                                'Jika, Ketika', 'Para, Ini, Itu', 'Wah, Aduh, Oi', 'Pertama, Kedua', 'Bertiga',\n",
    "                                'Satu, Dua', 'Beberapa', 'Saya, Kamu', 'Apa, Siapa', 'Kedua-duanya',\n",
    "                                'Sini, Situ, Sana', 'Bukan, Tidak', '@#$%^&', 'Pun, Kah', 'Foreign, Word']})\n",
    "\n",
    "df = df[['POS', 'POS Name', 'Example']]\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembagian Tagging\n",
    "Dibagi menjadi 2 yaitu **Class Content** dan **Class Function**.\n",
    "\n",
    "**Class Content:**\n",
    "1. JJ = Adjective\n",
    "2. NN = Common Noun\n",
    "3. NNP = Proper Noun\n",
    "4. NNG = Genitive Noun\n",
    "5. VBI = Intransitive Verb\n",
    "6. VBT = Transitive Verb\n",
    "7. FW = Foreign Word\n",
    "\n",
    "\n",
    "**Class Function:**\n",
    "1. OP = Open Parenthesis\n",
    "2. CP = Close Parenthesis\n",
    "3. GM = Slash\n",
    "4. ; = Semicolon\n",
    "5. : = Colon\n",
    "6. \" = Quotation\n",
    "7. . = Sentence Terminator\n",
    "8. , = Comma\n",
    "9. '-' = Dash\n",
    "10. ... = Ellipsis\n",
    "11. RB = Adverb\n",
    "12. IN = Preposition\n",
    "13. MD = Modal\n",
    "14. CC = Coor-Conjunction\n",
    "15. SC = Subor-Conjunction\n",
    "16. DT = Determiner\n",
    "17. UH = Interjection\n",
    "18. CDO = Ordinal Numerals\n",
    "19. CDC = Collective Numerals\n",
    "20. CDP = Primary Numerals\n",
    "21. CDI = Irregular Numerals\n",
    "22. PRP = Personal Pronouns\n",
    "23. WP = WH-Pronouns\n",
    "24. PRN = Number Pronouns\n",
    "25. PRL = Locative Pronouns\n",
    "26. NEG = Negation\n",
    "27. SYM = Symbol\n",
    "28. RP = Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ccon = ['JJ', 'NN','NNP', 'NNG', 'VBI', 'VBT', 'FW']\n",
    "Cfunc = ['OP', 'CP', 'GM', ';', ':', '\"', '.', \n",
    "         ',', '-', '...', 'RB', 'IN', 'MD', 'CC',\n",
    "         'SC', 'DT', 'UH', 'CDO', 'CDC', 'CDP', 'CDI',\n",
    "         'PRP', 'WP', 'PRN', 'PRL', 'NEG', 'SYM', 'RP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    String fileLexicon\n",
    "    String fileNGram\n",
    "    int NGramType = 0\n",
    "    int maxAffixLength = 3\n",
    "    int Treshold = 3\n",
    "    int minWordFreq = 0\n",
    "    int modeAffixTree = 0\n",
    "    boolean debug = False\n",
    "    double LambdaBigram = 0.2\n",
    "    int TwoPhaseType = 0\n",
    "    double beamFactor = 500.0\n",
    "    int useLexicon = 0\n",
    "\"\"\"\n",
    "\n",
    "def init_tag():\n",
    "    global mt\n",
    "    try:\n",
    "        if mt is None:\n",
    "            mt = MainTagger(\"resource/Lexicon.trn\", \"resource/Ngram.trn\", 0, 3, 3, 0, 0, False, 0.2, 0, 500.0, 1)\n",
    "    except:\n",
    "        print(\"Error Exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Saya suka makan bakso H. Hadi. Baksonya sangat enak dan mantap. Aku kepedesan hehe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sentence_extraction(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in a:\n",
    "    hasil = \" \".join(tokenisasi_kalimat(_)).strip()\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisasi dan tagging\n",
    "tagged_doc = []\n",
    "for doc in documents:\n",
    "    lines = doc.strip().split(\"\\n\")\n",
    "    try:\n",
    "        init_tag()\n",
    "        for l in lines:\n",
    "            if len(l) == 0: continue\n",
    "            out = sentence_extraction(cleaning(l))\n",
    "            for o in out:\n",
    "                strtag = \" \".join(tokenisasi_kalimat(o)).strip()\n",
    "#                 result += [\" \".join(mt.taggingStr(strtag))]\n",
    "                tagged_doc += [mt.taggingStr(strtag)]\n",
    "    except:\n",
    "        print (\"Error Exception\")\n",
    "\n",
    "for _ in tagged_doc:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tagging buat cari topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_con = []\n",
    "for tagged in tagged_doc:\n",
    "    con = []\n",
    "    for _ in tagged:\n",
    "        if _.split(\"/\", 1)[1] in Ccon:\n",
    "            con += [\"\".join(_)]\n",
    "    doc_con += [con]\n",
    "\n",
    "for _ in doc_con:\n",
    "    print (_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for con in doc_con:\n",
    "    co = []\n",
    "    for c in con:\n",
    "        result = c.split('/', 1)[0]\n",
    "        co.append(result)\n",
    "    documents += [co]\n",
    "    \n",
    "for _ in documents:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'name/NN'\n",
    "sep = '/'\n",
    "rest = text.split(sep, 1)[0]\n",
    "print (rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string=\"mengatakan/VBI\"\n",
    "print (my_string.split(\"/\",1)[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()     # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                      # return the smallest i such that\n",
    "        if rnd <= 0: return i         # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each documents\n",
    "document_lengths = [len(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of distinct words\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of documents\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "    \n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fration of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "    \n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "    \n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic, in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            \n",
    "            # remove this word / topic from counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print (k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = [\"Topik 1: \",\n",
    "               \"Topik 2: \",\n",
    "               \"Topik 3: \",\n",
    "               \"Topik 4: \",\n",
    "               \"Topik 5: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count, in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "topics = {'type': 'corpus',\n",
    "          'name': 'corpus1',\n",
    "          'docs': [{'name':'doc1',\n",
    "                   'topics':[{'name': 'docker' ,\n",
    "                              'words':['docker', 'containers', 'virtualidanzation']},\n",
    "                             {'name': 'windows' ,\n",
    "                              'words':['windows', 'machine', 'virtualization']},\n",
    "                             {'name': 'linux' ,\n",
    "                              'words':['linux', 'machine', 'virtualization']}\n",
    "                            ]\n",
    "                  },\n",
    "                  {'name':'doc2',\n",
    "                   'topics':[{'name': 'arraylist' ,\n",
    "                              'words':['arraylist', 'object', 'size']},\n",
    "                             {'name': 'java' ,\n",
    "                              'words':['java', 'arraylist']}\n",
    "                            ]\n",
    "                  }]\n",
    "          }\n",
    "          \n",
    "dicti = {'doc1': {'docker' : ['docker', 'containers', 'virtualization'],\n",
    "                    'windows' : ['windows', 'machine', 'virtualization'],\n",
    "                    'linux' : ['linux', 'machine', 'virtualization']\n",
    "                    },\n",
    "          'doc2': {'arraylist' : ['arraylist', 'object', 'size'],\n",
    "                    'java' : ['java', 'arraylist']\n",
    "                    }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents2 = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c41275d93467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Python\\POSLDA-Storytelling-Twitter\\lda\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, K, alpha, beta)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                         \u001b[1;31m# choose a new topic based on the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                         \u001b[0mnew_topic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_new_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                         \u001b[0mdocument_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\POSLDA-Storytelling-Twitter\\lda\\ldamodel.py\u001b[0m in \u001b[0;36mchoose_new_topic\u001b[1;34m(self, d, word, K)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         return self.sample_from([self.topic_weight(d, word, k)\n\u001b[1;32m--> 108\u001b[1;33m                             for k in range(K)])\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\POSLDA-Storytelling-Twitter\\lda\\ldamodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         return self.sample_from([self.topic_weight(d, word, k)\n\u001b[1;32m--> 108\u001b[1;33m                             for k in range(K)])\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\POSLDA-Storytelling-Twitter\\lda\\ldamodel.py\u001b[0m in \u001b[0;36mtopic_weight\u001b[1;34m(self, d, word, k)\u001b[0m\n\u001b[0;32m     91\u001b[0m         return the weight for the k-th topic\"\"\"\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_word_given_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_topic_given_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\POSLDA-Storytelling-Twitter\\lda\\ldamodel.py\u001b[0m in \u001b[0;36mp_word_given_topic\u001b[1;34m(self, word, topic, beta)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         return ((self.topic_word_counts[topic][word] + beta) /\n\u001b[1;32m---> 87\u001b[1;33m                 (self.topic_counts[topic] + W * beta))\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtopic_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "lda = LdaModel(documents2, 5, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
