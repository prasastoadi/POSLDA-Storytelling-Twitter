{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brokoli bagus untuk dimakan. Adikku suka makan brokoli, tetapi ibuku tidak.\"\n",
    "doc_b = \"Ibuku menghabiskan banyak waktu berkeliling melihat latihan bisbol adikku.\"\n",
    "doc_c = \"Beberapa ahli kesehatan menyarankan bahwa mengemudi dapat menyebabkan ketegangan dan tekanan darah meningkat.\"\n",
    "doc_d = \"Saya sering merasakan tekanan untuk tampil baik saat presentasi di sekolah.\"\n",
    "doc_e = \"Profesional kesehatan mengatakan bahwa brokoli itu baik untuk kesehatan Anda.\"\n",
    "doc_f = \"Teman saya seorang pemain bisbol yang pernah mendapatkan juara.\"\n",
    "doc_g = \"Pemain bisbol yang bernama Flash itu sangat suka memakan brokoli.\"\n",
    "doc_h = \"Sopir yang mengemudi taksi itu mendapatkan tekanan dari penumpangnya.\"\n",
    "doc_i = \"Saat tanding, olahraga bisbol memberikan ketegangan dan meningkatkan tekanan darah para penonton.\"\n",
    "doc_j = \"Ibuku menyarankan saya untuk memakan brokoli agar tekanan darah terkontrol.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from hmmtagger.tagger import MainTagger\n",
    "from tokenization import *\n",
    "\n",
    "mt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daftar Tagging\n",
    "Berikut adalah daftar tagging berdasarkan jurnal **HMM Based Part-of-Speech Tagger for Bahasa Indonesia.** (Alfan Farizki & Ayu Purwarianti, 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daftar tagging\n",
    "df = pd.DataFrame({'POS' : ['OP', 'CP', 'GM', ';', ':', \n",
    "                            '\"', '.', ',', '-', '...', \n",
    "                            'JJ', 'RB', 'NN', 'NNP','NNG', \n",
    "                            'VBI', 'VBT', 'IN', 'MD', 'CC', \n",
    "                            'SC', 'DT', 'UH', 'CDO', 'CDC', \n",
    "                            'CDP', 'CDI', 'PRP', 'WP', 'PRN', \n",
    "                            'PRL', 'NEG', 'SYM', 'RP', 'FW'],\n",
    "                   'POS Name' : ['Open Parenthesis', 'Close Parenthesis', 'Slash', 'Semicolon', 'Colon', \n",
    "                                 'Quotation', 'Sentence Terminator', 'Comma', 'Dash', 'Ellipsis', \n",
    "                                 'Adjective', 'Adverb', 'Common Noun', 'Proper Noun', 'Genitive Noun', \n",
    "                                 'Intransitive Verb', 'Transitive Verb', 'Preposition', 'Modal', 'Coor-Conjuction', \n",
    "                                 'Subor-Conjunction', 'Determiner', 'Interjection', 'Ordinal Numerals', 'Collective Numerals', \n",
    "                                 'Primary Numerals', 'Irregular Numerals', 'Personal Pronouns', 'WH-Pronouns', 'Number Pronouns',\n",
    "                                 'Locative Pronouns', 'Negation', 'Symbol', 'Particles', 'Foreign Word'],\n",
    "                   'Example' : ['({[', ')}]', '/', ';', ':',\n",
    "                                '\"', '.!?', ',', '-', '...',\n",
    "                                'Kaya, Manis', 'Sementara, Nanti', 'Mobil', 'Bekasi, Indonesia', 'Bukunya',\n",
    "                                'Pergi', 'Membeli', 'Di, Ke, Dari', 'Bisa', 'Dan, Atau, Tetapi',\n",
    "                                'Jika, Ketika', 'Para, Ini, Itu', 'Wah, Aduh, Oi', 'Pertama, Kedua', 'Bertiga',\n",
    "                                'Satu, Dua', 'Beberapa', 'Saya, Kamu', 'Apa, Siapa', 'Kedua-duanya',\n",
    "                                'Sini, Situ, Sana', 'Bukan, Tidak', '@#$%^&', 'Pun, Kah', 'Foreign, Word']})\n",
    "\n",
    "df = df[['POS', 'POS Name', 'Example']]\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembagian Tagging\n",
    "Dibagi menjadi 2 yaitu **Class Content** dan **Class Function**.\n",
    "\n",
    "**Class Content:**\n",
    "1. JJ = Adjective\n",
    "2. NN = Common Noun\n",
    "3. NNP = Proper Noun\n",
    "4. NNG = Genitive Noun\n",
    "5. VBI = Intransitive Verb\n",
    "6. VBT = Transitive Verb\n",
    "7. FW = Foreign Word\n",
    "\n",
    "\n",
    "**Class Function:**\n",
    "1. OP = Open Parenthesis\n",
    "2. CP = Close Parenthesis\n",
    "3. GM = Slash\n",
    "4. ; = Semicolon\n",
    "5. : = Colon\n",
    "6. \" = Quotation\n",
    "7. . = Sentence Terminator\n",
    "8. , = Comma\n",
    "9. '-' = Dash\n",
    "10. ... = Ellipsis\n",
    "11. RB = Adverb\n",
    "12. IN = Preposition\n",
    "13. MD = Modal\n",
    "14. CC = Coor-Conjunction\n",
    "15. SC = Subor-Conjunction\n",
    "16. DT = Determiner\n",
    "17. UH = Interjection\n",
    "18. CDO = Ordinal Numerals\n",
    "19. CDC = Collective Numerals\n",
    "20. CDP = Primary Numerals\n",
    "21. CDI = Irregular Numerals\n",
    "22. PRP = Personal Pronouns\n",
    "23. WP = WH-Pronouns\n",
    "24. PRN = Number Pronouns\n",
    "25. PRL = Locative Pronouns\n",
    "26. NEG = Negation\n",
    "27. SYM = Symbol\n",
    "28. RP = Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ccon = ['JJ', 'NN','NNP', 'NNG', 'VBI', 'VBT', 'FW']\n",
    "Cfunc = ['OP', 'CP', 'GM', ';', ':', '\"', '.', \n",
    "         ',', '-', '...', 'RB', 'IN', 'MD', 'CC',\n",
    "         'SC', 'DT', 'UH', 'CDO', 'CDC', 'CDP', 'CDI',\n",
    "         'PRP', 'WP', 'PRN', 'PRL', 'NEG', 'SYM', 'RP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    String fileLexicon\n",
    "    String fileNGram\n",
    "    int NGramType = 0\n",
    "    int maxAffixLength = 3\n",
    "    int Treshold = 3\n",
    "    int minWordFreq = 0\n",
    "    int modeAffixTree = 0\n",
    "    boolean debug = False\n",
    "    double LambdaBigram = 0.2\n",
    "    int TwoPhaseType = 0\n",
    "    double beamFactor = 500.0\n",
    "    int useLexicon = 0\n",
    "\"\"\"\n",
    "\n",
    "def init_tag():\n",
    "    global mt\n",
    "    try:\n",
    "        if mt is None:\n",
    "            mt = MainTagger(\"resource/Lexicon.trn\", \"resource/Ngram.trn\", 0, 3, 3, 0, 0, False, 0.2, 0, 500.0, 1)\n",
    "    except:\n",
    "        print(\"Error Exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Saya suka makan bakso H. Hadi. Baksonya sangat enak dan mantap. Aku kepedesan hehe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sentence_extraction(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in a:\n",
    "    hasil = \" \".join(tokenisasi_kalimat(_)).strip()\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisasi dan tagging\n",
    "tagged_doc = []\n",
    "for doc in documents:\n",
    "    lines = doc.strip().split(\"\\n\")\n",
    "    try:\n",
    "        init_tag()\n",
    "        for l in lines:\n",
    "            if len(l) == 0: continue\n",
    "            out = sentence_extraction(cleaning(l))\n",
    "            for o in out:\n",
    "                strtag = \" \".join(tokenisasi_kalimat(o)).strip()\n",
    "#                 result += [\" \".join(mt.taggingStr(strtag))]\n",
    "                tagged_doc += [mt.taggingStr(strtag)]\n",
    "    except:\n",
    "        print (\"Error Exception\")\n",
    "\n",
    "for _ in tagged_doc:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tagging buat cari topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_con = []\n",
    "for tagged in tagged_doc:\n",
    "    con = []\n",
    "    for _ in tagged:\n",
    "        if _.split(\"/\", 1)[1] in Ccon:\n",
    "            con += [\"\".join(_)]\n",
    "    doc_con += [con]\n",
    "\n",
    "for _ in doc_con:\n",
    "    print (_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for con in doc_con:\n",
    "    co = []\n",
    "    for c in con:\n",
    "        result = c.split('/', 1)[0]\n",
    "        co.append(result)\n",
    "    documents += [co]\n",
    "    \n",
    "for _ in documents:\n",
    "    print (_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'name/NN'\n",
    "sep = '/'\n",
    "rest = text.split(sep, 1)[0]\n",
    "print (rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string=\"mengatakan/VBI\"\n",
    "print (my_string.split(\"/\",1)[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = [\"Topik 1: \",\n",
    "               \"Topik 2: \",\n",
    "               \"Topik 3: \",\n",
    "               \"Topik 4: \",\n",
    "               \"Topik 5: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count, in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "topics = {'type': 'corpus',\n",
    "          'name': 'corpus1',\n",
    "          'docs': [{'name':'doc1',\n",
    "                   'topics':[{'name': 'docker' ,\n",
    "                              'words':['docker', 'containers', 'virtualidanzation']},\n",
    "                             {'name': 'windows' ,\n",
    "                              'words':['windows', 'machine', 'virtualization']},\n",
    "                             {'name': 'linux' ,\n",
    "                              'words':['linux', 'machine', 'virtualization']}\n",
    "                            ]\n",
    "                  },\n",
    "                  {'name':'doc2',\n",
    "                   'topics':[{'name': 'arraylist' ,\n",
    "                              'words':['arraylist', 'object', 'size']},\n",
    "                             {'name': 'java' ,\n",
    "                              'words':['java', 'arraylist']}\n",
    "                            ]\n",
    "                  }]\n",
    "          }\n",
    "          \n",
    "dicti = {'doc1': {'docker' : ['docker', 'containers', 'virtualization'],\n",
    "                    'windows' : ['windows', 'machine', 'virtualization'],\n",
    "                    'linux' : ['linux', 'machine', 'virtualization']\n",
    "                    },\n",
    "          'doc2': {'arraylist' : ['arraylist', 'object', 'size'],\n",
    "                    'java' : ['java', 'arraylist']\n",
    "                    }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents2 = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(documents2, 4, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n"
     ]
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Topik a:  7\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Topik b:  5\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Topik b:  2\n",
      "Topik c:  2\n",
      "Topik d:  2\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "Topik d:  3\n",
      "Topik c:  2\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "Topik c:  2\n",
      "Topik b:  2\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Topik c:  3\n",
      "Topik a:  3\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "Topik d:  3\n",
      "Topik c:  1\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "Topik c:  2\n",
      "Topik b:  2\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Topik b:  3\n",
      "Topik a:  1\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Topik a:  4\n",
      "['statistics', 'R', 'statsmodels']\n",
      "Topik d:  3\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "Topik d:  3\n",
      "Topik a:  1\n",
      "['pandas', 'R', 'Python']\n",
      "Topik d:  3\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Topik b:  5\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "Topik c:  3\n"
     ]
    }
   ],
   "source": [
    "lda.print_topics2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
